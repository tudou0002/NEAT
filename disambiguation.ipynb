{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, unicodedata\n",
    "from evaluate import *\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from extractors.entity import Entity\n",
    "import truecase\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "        Preprocesses the text: expanding contractions, removing emojis and punctuation marks\n",
    "    Args:\n",
    "        text (str): the text to be preprocessed\n",
    "    Returns:\n",
    "        str: the text after being preprocessed\n",
    "    \"\"\"\n",
    "    CONTRACTION_MAP = {\n",
    "        'names': 'name is',\n",
    "        'its': 'it is',\n",
    "        \"I'm\": \"I am\",\n",
    "        \"i'm\": \"I am\",\n",
    "        \"name's\": \"name is\",\n",
    "        \"it's\": \"it is\",  \n",
    "        \"I've\":\"I have\",\n",
    "        \"i've\": \"I have\",\n",
    "        \"we've\":'We have'\n",
    "    }\n",
    "\n",
    "    EMOJI_PATTERN = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "        \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "        \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "        \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "        \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "        \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        \"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\"\n",
    "    )\n",
    "\n",
    "    def replace_contraction(text):\n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(CONTRACTION_MAP.keys())), \n",
    "                                        flags=re.IGNORECASE)\n",
    "        def expand_match(contraction):\n",
    "            match = contraction.group(0)\n",
    "            first_char = match[0]\n",
    "            expanded_contraction = CONTRACTION_MAP.get(match)\\\n",
    "                                    if CONTRACTION_MAP.get(match)\\\n",
    "                                    else CONTRACTION_MAP.get(match.lower())                       \n",
    "            expanded_contraction = first_char+expanded_contraction[1:]\n",
    "            return expanded_contraction\n",
    "\n",
    "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "        return expanded_text\n",
    "\n",
    "\n",
    "    # return empty string if text is NaN\n",
    "    if type(text)==float:\n",
    "        return ''\n",
    "    # remove emoji\n",
    "    text = re.sub(EMOJI_PATTERN, r' ', text)\n",
    "    text = re.sub(r'·', ' ', text)\n",
    "    # convert non-ASCII characters to utf-8\n",
    "    text = unicodedata.normalize('NFKD',text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = replace_contraction(text)\n",
    "    text = re.sub(r'[\\'·\\\"”#$%&’()*+/:;<=>@[\\]^_`{|}~-]+',' ',text)\n",
    "    text = re.sub(r'[!,.?]{2,}\\s?',' ',text)\n",
    "    text = re.sub(r'[\\s]+',' ',text)\n",
    "    text = truecase.get_true_case(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "1. mask\n",
    "2. context+word vector -> knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"ht_bert/vocab.json\",\n",
    "    \"ht_bert/merges.txt\",\n",
    "    # model_max_length = 120,  \n",
    ")\n",
    "\n",
    "tokenizer_v2 = ByteLevelBPETokenizer(\n",
    "    \"ht_bert_v2/vocab.json\",\n",
    "    \"ht_bert_v2/merges.txt\",\n",
    "    # model_max_length = 120,  \n",
    ")\n",
    "\n",
    "tokenizer_v3 = ByteLevelBPETokenizer(\n",
    "    \"ht_bert_v3/vocab.json\",\n",
    "    \"ht_bert_v3/merges.txt\",\n",
    "    # model_max_length = 120,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "tokenizer_v2._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer_v2.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer_v2.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer_v2.enable_truncation(max_length=512)\n",
    "\n",
    "tokenizer_v3._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer_v3.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer_v3.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer_v3.enable_truncation(max_length=512)\n",
    "\n",
    "# print(tokenizer.encode(\"Hi im Emma, tonight my sexy friend Brielle will be joing the fun.\"))\n",
    "\n",
    "# print(tokenizer.encode(\"Hi im Emma, tonight my sexy friend Brielle will be joing the fun.\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"ht_bert\",\n",
    "    tokenizer=\"ht_bert\",\n",
    "    top_k=15,\n",
    ")\n",
    "\n",
    "fill_mask_v2 = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"ht_bert_v2\",\n",
    "    tokenizer=\"ht_bert_v2\",\n",
    "    top_k=15,\n",
    ")\n",
    "\n",
    "fill_mask_v3 = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"ht_bert_v3\",\n",
    "    tokenizer=\"ht_bert_v3\",\n",
    "    top_k=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask_v3(\"Hi my name is <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from extractors.name_extractor import NameExtractor\n",
    "\n",
    "df = pd.read_csv('data/crowdsource_filtered_ne_roberta_2.tsv', sep='\\t')\n",
    "# df = pd.read_csv('data_m/CanadaMax80_results.tsv', sep='\\t')\n",
    "name_df = pd.read_csv('extractors/src/nameslist.csv')\n",
    "name_set = set([name.lower().strip() for name in name_df['Name']])\n",
    "ne = NameExtractor(primary=['dict'], backoff=['rule'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, d in zip(df['title'], df['description']):\n",
    "    print(t)\n",
    "    print(d)\n",
    "    print()\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the ratio in the result\n",
    "def compute_ratio(fill_mask_sim, ne_result):\n",
    "    in_dict_counter = 0\n",
    "    total = len(fill_mask_sim)\n",
    "    for r in fill_mask_sim:\n",
    "        if r['token_str'].strip('Ġ').lower() in name_set:\n",
    "            in_dict_counter += 1\n",
    "        if r['token_str'].strip('Ġ').lower() == ne_result:\n",
    "            total -= 1\n",
    "            in_dict_counter -= 1\n",
    "            \n",
    "\n",
    "    return in_dict_counter / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sigma(fill_mask_result, word):\n",
    "    scores = [result['score'] for result in fill_mask_result]\n",
    "    \n",
    "    return np.std(np.array(scores))\n",
    "\n",
    "def compute_range(fill_mask_result, word):\n",
    "    scores = [result['score'] for result in fill_mask_result]\n",
    "    \n",
    "    return max(scores) - min(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "# NLP = English()\n",
    "# sentencizer = NLP.create_pipe(\"sentencizer\")\n",
    "# NLP.add_pipe(sentencizer)\n",
    "\n",
    "def disambiguate_layer(context, entities, fill_mask, window_size=5):\n",
    "    '''\n",
    "    context: the sentence that include the query word\n",
    "    entities: results from the extractor, a list of entities\n",
    "\n",
    "    return: a list of entities with modified fill_mask_conf and fill_mask_std fields\n",
    "    '''\n",
    "    # sanity check\n",
    "    if not entities:\n",
    "        return []\n",
    "    # preprocess the context, docode\n",
    "    cleaned_context = preprocess(context)\n",
    "\n",
    "    results = []\n",
    "    for ent in entities:\n",
    "        info_dict = {}\n",
    "        word = ent.text\n",
    "        info_dict['word'] = word.lower()\n",
    "\n",
    "        # select the context window for the word\n",
    "        context_list = cleaned_context.lower().split()\n",
    "        try:\n",
    "            word_idx = context_list.index(word.lower())\n",
    "        except:\n",
    "#             print(context_list, word)\n",
    "            return {}\n",
    "        window = ' '.join(context_list[max(0,word_idx-window_size):min(len(context_list), word_idx+window_size)])\n",
    "        \n",
    "        window = window.replace(word.lower(), '<mask>',1)\n",
    "        info_dict['context'] = window\n",
    "\n",
    "        fill_mask_sim = fill_mask(window)\n",
    "        ratio = compute_ratio(fill_mask_sim, word.lower())\n",
    "        info_dict['ratio'] = ratio\n",
    "        results.append(info_dict)\n",
    "        \n",
    "        ent.fill_mask_conf = ratio\n",
    "        ent.fill_mask_std = (compute_sigma(fill_mask_sim, word), compute_range(fill_mask_sim, word))\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### an experiment \n",
    "Goal: to check if the standard deviation of true names and non-names are different (distribution and avg)\n",
    "attr: score, std, max, (min)\n",
    "Step: \n",
    "1. split  the dataset randomly into 80/20\n",
    "2. compute the sigma for each candidates extracted by the extarctor, store in the std field\n",
    "3. group the result by whether it's ture result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "disambiguated_entities = []\n",
    "for idx in range(len(df[:100])):\n",
    "    cnt = df['description'][idx]+' '+df['title'][idx]\n",
    "    result_entity = ne.extract(cnt)\n",
    "    results = disambiguate_layer(cnt, result_entity, fill_mask)\n",
    "    disambiguated_entities.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with the true names\n",
    "correct = []  #  a list of correctly identified names\n",
    "incorrect = []\n",
    "\n",
    "for idx in range(len(df[:100])):\n",
    "    true = literal_eval(df['True'][idx])\n",
    "    sub_correct = [ent for ent in disambiguated_entities[idx] if ent.text.lower() in true]\n",
    "    sub_incorrect = [ent for ent in disambiguated_entities[idx] if ent.text.lower() not in true]\n",
    "    correct.append(sub_correct)\n",
    "    incorrect.append(sub_incorrect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([ent.fill_mask_std for ents in correct for ent in ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([ent.fill_mask_std for ents in incorrect for ent in ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "correct_data = {'text':[ent.text for ents in correct for ent in ents],\n",
    "                'std':[ent.fill_mask_std[0] for ents in correct for ent in ents],\n",
    "                'range':[ent.fill_mask_std[1] for ents in correct for ent in ents],\n",
    "                'score':[ent.fill_mask_conf for ents in correct for ent in ents]}\n",
    "fig = px.scatter(correct_data, x=\"std\",y='score', hover_data=['text','std','score'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_data = {'text':[ent.text for ents in incorrect for ent in ents],\n",
    "                'std':[ent.fill_mask_std[0] for ents in incorrect for ent in ents],\n",
    "                  'range':[ent.fill_mask_std[1] for ents in incorrect for ent in ents],\n",
    "                'score':[ent.fill_mask_conf for ents in incorrect for ent in ents]}\n",
    "# fig = px.scatter(incorrect_data, x=\"std\",y='score', hover_data=['text','std', 'score'])\n",
    "# fig.show()\n",
    "total_data = {'text':correct_data['text']+incorrect_data['text'],\n",
    "              'std': correct_data['std']+incorrect_data['std'],\n",
    "              'range':correct_data['range']+incorrect_data['range'],\n",
    "              'score': correct_data['score']+incorrect_data['score'],\n",
    "             'label':['correct' for i in range(len(correct_data['text']))]+['incorrect' for i in range(len(incorrect_data['text']))]}\n",
    "\n",
    "fig = px.scatter_matrix(total_data, dimensions=[\"std\", \"range\", \"score\"],color='label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experiment 2 \n",
    "check if the first returned prediction is the same as the word we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_top_pred(context, entities, fill_mask, window_size=5):\n",
    "    '''\n",
    "    context: the sentence that include the query word\n",
    "    entities: results from the extractor, a list of entities\n",
    "\n",
    "    return: a list of entities with modified fill_mask_conf and fill_mask_std fields\n",
    "    '''\n",
    "    # sanity check\n",
    "    if not entities:\n",
    "        return []\n",
    "    # preprocess the context, docode\n",
    "    cleaned_context = preprocess(context)\n",
    "\n",
    "    results = []\n",
    "    for ent in entities:\n",
    "        info_dict = {}\n",
    "        word = ent.text\n",
    "        info_dict['word'] = word.lower()\n",
    "\n",
    "        # select the context window for the word\n",
    "        context_list = cleaned_context.lower().split()\n",
    "        try:\n",
    "            word_idx = context_list.index(word.lower())\n",
    "        except:\n",
    "#             print(context_list, word)\n",
    "            return {}\n",
    "        window = ' '.join(context_list[max(0,word_idx-window_size):min(len(context_list), word_idx+window_size)])\n",
    "        \n",
    "        window = window.replace(word.lower(), '<mask>',1)\n",
    "        info_dict['context'] = window\n",
    "\n",
    "        fill_mask_sim = fill_mask(window)\n",
    "        # confidence score is set to be if the masked word is the same as the predicted word\n",
    "        # 0: unlikely to be a name since the context is highly possible for a particular word\n",
    "        if word in [fill_mask_sim[i]['token_str'].strip('Ġ').lower() for i in range(3)] :\n",
    "            conf = 0\n",
    "            print(window)\n",
    "        else:\n",
    "            conf = 1\n",
    "        \n",
    "        \n",
    "        ent.fill_mask_conf = conf\n",
    "        ent.fill_mask_std = (compute_sigma(fill_mask_sim, word), compute_range(fill_mask_sim, word))\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "disambiguated_entities = []\n",
    "for idx in range(len(df)):\n",
    "    cnt = df['description'][idx]+' '+df['title'][idx]\n",
    "    result_entity = ne.extract(cnt)\n",
    "    results = check_top_pred(cnt, result_entity, fill_mask_v3)\n",
    "    disambiguated_entities.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the entity if the confidence is 0\n",
    "for ents, idx in zip(disambiguated_entities, range(len(df))):\n",
    "    for ent in ents:\n",
    "        if ent.fill_mask_conf==0:\n",
    "            print('='*100)\n",
    "            print(ent.text)\n",
    "            cnt = df['description'][idx]+' '+df['title'][idx]\n",
    "            print(cnt)\n",
    "            print(ent.fill_mask_conf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask_v3('.! hi my name is <mask> . . im sexy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []  #  a list of correctly identified names\n",
    "incorrect = []\n",
    "\n",
    "for idx in range(len(df[:100])):\n",
    "    true = literal_eval(df['True'][idx])\n",
    "    sub_correct = [ent for ent in disambiguated_entities[idx] if ent.text.lower() in true]\n",
    "    sub_incorrect = [ent for ent in disambiguated_entities[idx] if ent.text.lower() not in true]\n",
    "    correct.append(sub_correct)\n",
    "    incorrect.append(sub_incorrect)\n",
    "\n",
    "print([ent.fill_mask_conf for ents in incorrect for ent in ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current options for disambiguation\n",
    "- ratio\n",
    "- if top prediction == masked word\n",
    "- std / range (doesn't perform well）\n",
    "- vector similarity (still exploring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "filtered_results_v1 = []\n",
    "for idx in range(len(df[:100])):\n",
    "#     ne_result = literal_eval(df['dict'][idx])\n",
    "    cnt = df['description'][idx]+' '+df['title'][idx]\n",
    "    result_entity = ne.extract(cnt)\n",
    "#     ne_result = [ent.text for ent in result_entity]\n",
    "    results = disambiguate_layer(cnt, result_entity, fill_mask)\n",
    "    filtered_result = []\n",
    "    for result in results:\n",
    "        if result['ratio']>0:\n",
    "            # print(result['context'])\n",
    "            # print(result['word'])\n",
    "            # print('-'*80)\n",
    "            filtered_result.append(result['word'])\n",
    "    filtered_results_v1.append(filtered_result)\n",
    "    \n",
    "filtered_results_v2 = []\n",
    "for idx in range(len(df[:100])):\n",
    "#     ne_result = literal_eval(df['dict'][idx])\n",
    "    cnt = df['description'][idx]+' '+df['title'][idx]\n",
    "    result_entity = ne.extract(cnt)\n",
    "#     ne_result = [ent.text for ent in result_entity]\n",
    "    results = disambiguate_layer(cnt, result_entity, fill_mask_v2)\n",
    "    filtered_result = []\n",
    "    for result in results:\n",
    "        if result['ratio']>0:\n",
    "            # print(result['context'])\n",
    "            # print(result['word'])\n",
    "            # print('-'*80)\n",
    "            filtered_result.append(result['word'])\n",
    "    filtered_results_v2.append(filtered_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, result in enumerate(filtered_results_v2):\n",
    "    if result:\n",
    "        print(df['description'][idx]+' '+df['title'][idx])\n",
    "        print('1st bert model:', filtered_results_v1[idx])\n",
    "        print('2nd bert model:', result)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(100):\n",
    "    cnt = df['description'][idx]+' '+df['title'][idx]\n",
    "    result_entity = ne.extract(cnt)\n",
    "    ne_result = [ent.text for ent in result_entity]\n",
    "    \n",
    "    if ne_result:\n",
    "        print(cnt)\n",
    "        print('Results from dict&rule extracotr:', ne_result)\n",
    "        print('1st bert model:', filtered_results_v1[idx])\n",
    "        print('2nd bert model:', filtered_results_v2[idx])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = fill_mask_v2('$180/hr Worth every <mask> Multi hr Specials!')\n",
    "# f = fill_mask_v2('I am doing duo with <mask> great reviews on') #lilia\n",
    "f = fill_mask_v2('THANKS for view :Im <mask>,friendly and happy latina') #karla\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_sigma(f, 'penny')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM-KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config).from_pretrained(\"ht_bert_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "model.lm_head = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embeddings = pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=model,\n",
    "    tokenizer=\"ht_bert_v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Hi im Emma, tonight my sexy friend Brielle will be joing the fun.'\n",
    "sents = [\"Amazing outcall with a blonde Latina bombshell 647-470-6038 Carmalita\",\n",
    "         \"Amazing outcall with a blonde Latina bombshell 647-470-6038 AMBER\"]\n",
    "# toks = tokenizer.encode(sents).tokens\n",
    "embeddings = get_embeddings(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
